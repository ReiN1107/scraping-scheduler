{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "id": "KIi0LjKX1j0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gspread oauth2client pandas"
      ],
      "metadata": {
        "id": "6p7aSeJdGxaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import time\n",
        "import csv\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Google Colab で Google ドライブをマウントする場合（今回は不要の場合はコメントアウト可）\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "def extract_card_data(card_html):\n",
        "    \"\"\"\n",
        "    1件分の募集カード HTML から各情報を抽出する関数。\n",
        "    取得項目:\n",
        "      - company: 企業名\n",
        "      - deadline_date: 締切日（例: 「本日」「明日」または \"2/16\"）\n",
        "      - deadline_time: 締切時間（例: \"17:00\" または \"23:59\"）\n",
        "      - content: 募集タイトル\n",
        "      - schedule: 日程\n",
        "      - location: 開催場所\n",
        "      - url: 募集詳細ページの URL\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(card_html, \"html.parser\")\n",
        "\n",
        "    # ① 企業名の抽出\n",
        "    company_div = soup.find(\"div\", class_=\"mantine-Text-root text-md text-black w-full cursor-pointer hover:opacity-50 pl-12 pr-8 line-clamp-2 mantine-1d564l0\")\n",
        "    company = company_div.get_text(strip=True) if company_div else \"\"\n",
        "\n",
        "    # ② 締切情報の抽出（2種類のパターンに対応）\n",
        "    deadline_date = \"\"\n",
        "    deadline_time = \"\"\n",
        "    # まず、bg-deadline-notifcation のコンテナを探す\n",
        "    deadline_container = soup.find(\"div\", class_=\"bg-deadline-notifcation\")\n",
        "    if deadline_container:\n",
        "        inner_div = deadline_container.find(\"div\", class_=\"mantine-1avyp1d\")\n",
        "        if inner_div:\n",
        "            spans = inner_div.find_all(\"span\")\n",
        "            if len(spans) >= 2:\n",
        "                deadline_date = spans[0].get_text(strip=True)\n",
        "                deadline_time = spans[1].get_text(strip=True)\n",
        "    else:\n",
        "        # 見つからなかった場合、text-deadline-primary のコンテナを探す\n",
        "        deadline_container = soup.find(\"div\", class_=\"text-deadline-primary\")\n",
        "        if deadline_container:\n",
        "            # ②の例では、内部に <div class=\"flex justify-center items-baseline mantine-1avyp1d\"> がある\n",
        "            inner_div = deadline_container.find(\"div\", class_=\"flex\")\n",
        "            if inner_div:\n",
        "                spans = inner_div.find_all(\"span\")\n",
        "                if len(spans) >= 3:\n",
        "                    # 1番目の span を締切日、3番目の span を締切時間とする（2番目は曜日など）\n",
        "                    deadline_date = spans[0].get_text(strip=True)\n",
        "                    deadline_time = spans[2].get_text(strip=True)\n",
        "                elif len(spans) >= 2:\n",
        "                    deadline_date = spans[0].get_text(strip=True)\n",
        "                    deadline_time = spans[1].get_text(strip=True)\n",
        "\n",
        "    # ③ 募集タイトル（内容）の抽出\n",
        "    content_div = soup.find(\"div\", class_=\"font-bold pc:text-md text-sm line-clamp-2\")\n",
        "    content = content_div.get_text(strip=True) if content_div else \"\"\n",
        "\n",
        "    # ④ 募集詳細ページの URL の抽出\n",
        "    # カード内の最初の <a> タグの href 属性を利用\n",
        "    a_tag = soup.find(\"a\", href=True)\n",
        "    url_value = \"\"\n",
        "    if a_tag:\n",
        "        url_value = a_tag[\"href\"]\n",
        "        if url_value.startswith(\"/\"):\n",
        "            url_value = \"https://gaishishukatsu.com\" + url_value\n",
        "\n",
        "    # ⑤ 日程と開催場所の抽出\n",
        "    schedule = \"\"\n",
        "    location = \"\"\n",
        "    parent_divs = soup.find_all(\"div\", class_=\"inline pc:text-base text-sm\")\n",
        "    for parent in parent_divs:\n",
        "        label_div = parent.find(\"div\", class_=\"border-[1px] border-black-2 rounded-full px-8 inline-block line-clamp-1 mr-8\")\n",
        "        if label_div:\n",
        "            label_text = label_div.get_text(strip=True)\n",
        "            value_div = parent.find(\"div\", class_=\"inline-block whitespace-normal line-clamp-1\")\n",
        "            if value_div:\n",
        "                if \"日程\" in label_text:\n",
        "                    schedule = value_div.get_text(strip=True)\n",
        "                elif \"場所\" in label_text:\n",
        "                    location = value_div.get_text(strip=True)\n",
        "\n",
        "    return {\n",
        "        \"company\": company,\n",
        "        \"deadline_date\": deadline_date,\n",
        "        \"deadline_time\": deadline_time,\n",
        "        \"content\": content,\n",
        "        \"schedule\": schedule,\n",
        "        \"location\": location,\n",
        "        \"url\": url_value\n",
        "    }\n",
        "\n",
        "def scrape_all_pages():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "\n",
        "    driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "    results = []\n",
        "    page = 1\n",
        "    while True:\n",
        "        url = f\"https://gaishishukatsu.com/recruiting_info?order=deadline&gy=2027&allgy=false&page={page}\"\n",
        "        print(f\"Processing page {page}: {url}\")\n",
        "        driver.get(url)\n",
        "\n",
        "        try:\n",
        "            WebDriverWait(driver, 15).until(\n",
        "                EC.presence_of_element_located(\n",
        "                    (By.CSS_SELECTOR, \"div.bg-white.rounded-\\\\[16px\\\\].shadow-card.mb-16.mantine-1avyp1d\")\n",
        "                )\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: ページ {page} で募集カード待機中エラー:\", e)\n",
        "\n",
        "        time.sleep(2)  # レンダリング待ち\n",
        "        html = driver.page_source\n",
        "        soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "        # 「一致する募集情報は見つからなかった」メッセージがある場合、ループ終了\n",
        "        no_info_div = soup.find(\"div\", class_=\"bg-white mb-[10px] px-20 py-[10px] text-base text-black-1 mantine-1avyp1d\")\n",
        "        if no_info_div and \"一致する募集情報は見つからなかった\" in no_info_div.get_text():\n",
        "            print(f\"ページ {page} で募集情報が見つからなかったためループ終了\")\n",
        "            break\n",
        "\n",
        "        # 募集カードの抽出\n",
        "        cards = soup.find_all(\"div\", class_=\"bg-white rounded-[16px] shadow-card mb-16 mantine-1avyp1d\")\n",
        "        print(f\"ページ {page} の募集カード数:\", len(cards))\n",
        "\n",
        "        # 募集カードが 0 件の場合は処理終了\n",
        "        if len(cards) == 0:\n",
        "            print(f\"ページ {page} で募集カードが見つからなかったため処理を終了します。\")\n",
        "            break\n",
        "\n",
        "        for card in cards:\n",
        "            card_html = str(card)\n",
        "            data = extract_card_data(card_html)\n",
        "            results.append(data)\n",
        "            print(\"書き込み:\", data)\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(1)\n",
        "\n",
        "    driver.quit()\n",
        "    return results\n",
        "\n",
        "def save_results_to_csv(results, csv_filename=\"recruitment_info_all_pages_gaishishukatsu.csv\"):\n",
        "    \"\"\"\n",
        "    指定のファイル名（保存先パスを指定しない場合は現在の作業ディレクトリ）に CSV を保存する\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
        "    print(\"CSV saved to:\", csv_filename)\n",
        "\n",
        "# メイン処理\n",
        "results = scrape_all_pages()\n",
        "save_results_to_csv(results)"
      ],
      "metadata": {
        "id": "VlLQMUi31FPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gspread\n",
        "import pandas as pd\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "def update_spreadsheet_from_csv():\n",
        "    \"\"\"\n",
        "    CSVファイル (recruitment_info_all_pages_gaishishukatsu.csv) の内容を\n",
        "    Google スプレッドシート「選考締め切り情報」の「外資就活」シートに反映する。\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 使用するスコープを定義\n",
        "    scope = [\n",
        "        \"https://spreadsheets.google.com/feeds\",\n",
        "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
        "        \"https://www.googleapis.com/auth/drive.file\",\n",
        "        \"https://www.googleapis.com/auth/drive\"\n",
        "    ]\n",
        "\n",
        "    # 2. credentials.json (サービスアカウントの認証情報) を使って認証\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(\"credentials.json\", scope)\n",
        "    client = gspread.authorize(creds)\n",
        "\n",
        "    # 3. 対象スプレッドシートとシートを取得\n",
        "    spreadsheet = client.open(\"選考締め切り情報\")  # スプレッドシート名\n",
        "    worksheet = spreadsheet.worksheet(\"外資就活\")  # シート名\n",
        "\n",
        "    # 4. CSVファイルを pandas で読み込み\n",
        "    csv_filename = \"recruitment_info_all_pages_gaishishukatsu.csv\"\n",
        "    df = pd.read_csv(csv_filename, encoding=\"utf-8-sig\")\n",
        "\n",
        "    # 5. DataFrame をリスト形式に変換（先頭にヘッダー行を含む）\n",
        "    data = [df.columns.tolist()] + df.values.tolist()\n",
        "\n",
        "    # 6. シートの内容をクリアし、新しいデータを書き込み\n",
        "    worksheet.clear()\n",
        "    worksheet.update(\"A1\", data)\n",
        "\n",
        "    print(\"スプレッドシート「選考締め切り情報」シート「外資就活」に CSV の内容が更新されました。\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    update_spreadsheet_from_csv()"
      ],
      "metadata": {
        "id": "z7Y1sqBXP5Q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}